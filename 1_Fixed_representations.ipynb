{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnTvFChmPyvKUO/h347bO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-AJ-H/AIMS26/blob/main/1_Fixed_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using deterministic representations to predicting polymer properties with neural networks\n",
        "\n",
        "In this workbook, we use RDKit to calculate a deterministic set of representations which can describe polymer structures. These are then passed to a simple neural network structure for predicting polymer properties.\n",
        "\n",
        "You will:\n",
        "   \n",
        "*   Explore how RDKit can be used to calculate molecular properties from SMILES strings\n",
        "*   Create representations of polymers using Morgan fingerprints\n",
        "*   Train a simple neural network which uses these representations to predict properties.\n",
        "\n",
        "In this example, we specifically look at predicting the \"electron affinity\" of polymers for photocatalytic water splitting. This value (\"EA\") is extremely important because it is approximately equal to the energy of photo-generated electrons participating in the water splitting reaction. The value of EA defines a necessary (but not sufficient) condition on a polymer's reaction efficiency.\n",
        "\n",
        "Resources:\n",
        ">RDKit:   \n",
        ">https://rdkit.org/docs/index.html\n",
        "\n",
        ">Data from:  \n",
        ">https://pubs.acs.org/doi/full/10.1021/jacs.9b03591\n"
      ],
      "metadata": {
        "id": "z0ks-jl0cpDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Install packages"
      ],
      "metadata": {
        "id": "_v8tr5FRJI0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RDKit (~30 sec)\n",
        "!pip install rdkit -qq\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Draw, DataStructs, rdFingerprintGenerator\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "# ML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, PredefinedSplit\n",
        "\n",
        "# Misc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import io"
      ],
      "metadata": {
        "id": "eQX-rEvBHdwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Import and explore data\n",
        "\n",
        "> poly_ID = MonomerA_MonomerB  \n",
        "> poly_SMI = polymer representation (A.B) in SMILES format.  \n",
        "> EA = electron affinity ~ energy of electrons participating in the reaction. This is the target we wish to predict.    \n",
        "> IP = ionisation potential = how much energy is required to fully eject an electron from the polymer. We won't use this today."
      ],
      "metadata": {
        "id": "Lu-ZY2UPJN8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwfsGfIML2ix"
      },
      "outputs": [],
      "source": [
        "# Get the polymer SMILES from GitHub.\n",
        "csv_url = \"https://raw.githubusercontent.com/S-AJ-H/AIMS26/25478252292fe3bde0e4fb06977ea21c7e05545a/dataset.csv\"\n",
        "df_data = pd.read_csv(csv_url)\n",
        "display(df_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Chemistry with RDKit\n",
        "\n",
        "> We construct 'molecule' objects from the SMILES using RDKit. RDKit uses a collection of rules to calculate a complete set of molecule-defining chemical information from the SMILES. These graph-based molecule objects encode the atomic structure, bonds, spatial arrangement etc of a molecule - we can then do digital chemistry with these!\n",
        "\n",
        "> We can represent each polymer *[-A-B-]n* in a single RDKit 'molecule' object as a pair of monomers *A.B* :"
      ],
      "metadata": {
        "id": "vKBE4wFjZKpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the SMILES to mol objects using MolFromSmiles on each element in the Series:\n",
        "df_chem = df_data.copy()\n",
        "random_mols_index = [0,1000,2000,5000, 6137]                                                  # look at some random polymers\n",
        "df_chem['poly_MOL'] = df_chem['poly_SMI'].iloc[random_mols_index].apply(Chem.MolFromSmiles)   # Chem.MolFromSmiles converts the SMILES into molecules\n",
        "\n",
        "# Draw the pairs:\n",
        "img = Draw.MolsToGridImage(list(df_chem.poly_MOL.iloc[random_mols_index]), molsPerRow=5)\n",
        "display(img)"
      ],
      "metadata": {
        "id": "7n4U7vFSZGhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As an example, lets calculate the partial charges for the last pair of monomers. The resulting graph shows where electrons are localised in the molecules (blue = higher electron density, red = electron deficient)."
      ],
      "metadata": {
        "id": "c7woyYPik48E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate partial charges\n",
        "mol = df_chem.poly_MOL.iloc[6137]                                               # molecule representing the final pair of monomers (as example)\n",
        "Chem.AllChem.ComputeGasteigerCharges(mol)                                       # calculate the partial charges\n",
        "charges = [x.GetDoubleProp('_GasteigerCharge') for x in mol.GetAtoms()]         # store the partial charges, with same indexing as atoms\n",
        "\n",
        "# plotting function\n",
        "def show_image(data):\n",
        "    bio = io.BytesIO(data)\n",
        "    img = Image.open(bio)\n",
        "    display(img)\n",
        "\n",
        "colours = [(0.0, 0.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0), (1.0, 0.0, 0.0, 1.0)]    # define colour range (min = blue, med = white, max = red)\n",
        "d = Draw.MolDraw2DCairo(400, 400)\n",
        "SimilarityMaps.GetSimilarityMapFromWeights(mol=mol,weights=charges,draw2d=d, colorMap=colours, contourLines=50)\n",
        "d.FinishDrawing()\n",
        "show_image(d.GetDrawingText())"
      ],
      "metadata": {
        "id": "Qeu0y7MnjhPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1.1 Question: Look at the partial charge graph. Can you see the downside to representing a polymer as a pair of monomers? Hint: construct a partial charge graph for this SMILES: \"Nc1cc(-c2ccc3c(c2)S(=O)(=O)c2ccccc2-3)ccc1\""
      ],
      "metadata": {
        "id": "0ITtU3FV91Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this box to help answer Question 1.1\n",
        "\n",
        "# calculate partial charges\n",
        "######################################\n",
        "# draw\n",
        "######################################"
      ],
      "metadata": {
        "id": "Tg24wN7wjbgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Generate a fixed representation for each material\n",
        "\n",
        "> To predict the property \"EA\" of our polymers, we need to generate a representation of each polymer which can be passed to a feed forward neural network. This representation is made up of \"fingerprints\"."
      ],
      "metadata": {
        "id": "LErd2d-pJWmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Understanding Morgan fingerprints"
      ],
      "metadata": {
        "id": "Ew3VeMjXnNAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> There are a number of well-established, deterministic ways of making fingerprints. In this example, we use the popular \"count Morgan fingerprints\", which calculate a set of molecule-defining chemical fragments.\n",
        "Count Morgan fingerprints are generated by iteratively encoding each atom’s local neighborhood up to a chosen radius into IDs. These are counted to record how many times each unique substructure occurs in the molecule. These identifiers are then hashed into positions of a fixed-length vector, with counts accumulated per index.\n",
        "\n",
        "* The molecule fragment is drawn with the atoms in the same positions as in the original molecule.\n",
        "* The original atom is highlighted in blue.\n",
        "* Aromatic atoms are highlighted in yellow.\n",
        "* Atoms/bonds that are drawn in light gray indicate parts of the structure which influence the atoms’ properties but are not directly part of the fingerprint (e.g. they might define whether an atom in in a ring or not)."
      ],
      "metadata": {
        "id": "PlA6laQnnTxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mol = df_chem.poly_MOL.iloc[6137]\n",
        "\n",
        "# Define Morgan fingerprints\n",
        "mfp = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)          # radius = \"look two atoms away from each atom\". fpSize = length of feature vector\n",
        "ao = rdFingerprintGenerator.AdditionalOutput()                                  # extract metadata for plotting\n",
        "ao.AllocateBitInfoMap()\n",
        "\n",
        "# Make fingerprints\n",
        "fp = mfp.GetFingerprint(mol,additionalOutput=ao)\n",
        "print(f\"Fingerprint Bit IDs: {list(fp.GetOnBits())}\\n\")\n",
        "\n",
        "# Draw last 12\n",
        "tpls = [(mol,x,ao.GetBitInfoMap()) for x in fp.GetOnBits()]\n",
        "print(f\"Number of fingerprints: {len(fp.GetOnBits())}\\n\")\n",
        "Draw.DrawMorganBits(tpls[:12],molsPerRow=4,legends=[str(x) for x in fp.GetOnBits()][:12])\n"
      ],
      "metadata": {
        "id": "ADrUJcihfclp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generating fingerprints for ML"
      ],
      "metadata": {
        "id": "vg6R8Q_BnIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function which calculates Morgan Fingerprints:\n",
        "def generate_fingerprints(smiles, radius, nBits):\n",
        "\n",
        "  # define fingerprints:\n",
        "  mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=nBits)\n",
        "  # Generate:\n",
        "  mols = [Chem.MolFromSmiles(smi) for smi in smiles]                    # convert the smiles in the df into a list of molecular graphs \"mols\"\n",
        "  individual_cfps = [mfpgen.GetCountFingerprint(mol) for mol in mols]   # Generate count Morgan fps\n",
        "  #individual_cfps = [mfpgen.GetFingerprint(mol) for mol in mols]       # binary Morgan fps\n",
        "\n",
        "  # \"individual_cfps\" are RDKit SparseIntVect objects which only store non-zero values.\n",
        "  # To convert to NumPy arrays, we need to iterate over all molecules and add in the zeros:\n",
        "  cfp_arrays = []\n",
        "  for fp in individual_cfps:                                            # for each individual set of fingerprints (i.e. for each mol)\n",
        "    array = np.zeros((nBits), dtype=int)                                # create an array of zeros of appropriate size\n",
        "    DataStructs.ConvertToNumpyArray(fp, array)                          # RDKit function which copies the fingerprint into the array.\n",
        "    cfp_arrays.append(array)                                            # store the fingerprints array in a list\n",
        "  cfp = np.array(cfp_arrays)                                            # Convert the list of arrays to a single 2D NumPy array\n",
        "  return cfp"
      ],
      "metadata": {
        "id": "IfG2gmPqKm0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define key parameters\n",
        "nBits=1024                            # default 1024 - total length of feature vector\n",
        "radius = 2                            # default 2 - how many adjacent atoms to consider, for each atom\n",
        "smiles = df_data.loc[:, 'poly_SMI']   # from our imported data\n",
        "\n",
        "# generate fingerprints\n",
        "fingerprints = ###############################  # call our function from above, return an array\n",
        "\n",
        "# Create a DataFrame combining our original data with newly generated fingerprints\n",
        "fingerprint_df = pd.DataFrame(fingerprints, columns=[f'bit-{x}' for x in range(nBits)])\n",
        "df = pd.concat([df_data, fingerprint_df], axis=1)\n",
        "display(df.head())\n",
        "# now we have the targets and the representations\n"
      ],
      "metadata": {
        "id": "lacWIyHhPkvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. Questions:\n",
        "  >Try switching between the count and binary Morgan fingerprint generators using 16 and 1024 nBits.    \n",
        "  >Look up the difference between count and binary Morgan fingerprints. Why might you use one over the other?  \n",
        "  > Why are 16 bits not enough to describe our polymers?"
      ],
      "metadata": {
        "id": "RJ-PNyyXny_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Machine learning\n",
        "\n",
        "> Now that we have representations that are ML-friendly, we can input them into a feedforward neural network and train the model to predict continuous target values."
      ],
      "metadata": {
        "id": "jf-0mY4SLYAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 3.1 Define network architecture and parameters\n"
      ],
      "metadata": {
        "id": "XgKQAEAvMvnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model architecture with two hidden layers:\n",
        "num_epochs = 30\n",
        "hidden_dim = 32\n",
        "hidden_dim_2 = 32\n",
        "dropout = 0.0\n",
        "lr=1e-4\n",
        "batch_size = 64\n",
        "criterion = nn.MSELoss()\n",
        "patience = 3                                                #for early stopping\n",
        "scaler = StandardScaler()                                   #rescale to zero mean and unit variance\n",
        "\n",
        "class SimpleNN(nn.Module):                                  #inherit from PyTorch Module class\n",
        "    def __init__(self, input_dim=nBits, hidden_dim=hidden_dim, dropout=dropout):\n",
        "        super(SimpleNN, self).__init__()                    #super calls nn.Module __init__ method, as there's lots in it\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_2, 1)\n",
        "        )\n",
        "    def forward(self, x):                                   #required as part of nn.Module, called automatically when running SimpleNN(x)\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "eqUGbgXvVRwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.2 Extract and split features and targets\n",
        "\n",
        "> Note: The training structure has designed to be easily adapted for K-fold cross-validation. In this workshop, we will train on the entire dataset, so any references to folds can be ignored."
      ],
      "metadata": {
        "id": "K9WPsh0JJW0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features (fingerprints) and targets (EA)\n",
        "X = ########################################                             # numpy array comprising all fingerprint columns [shape (6138,1024)]\n",
        "y = ########################################                                   # numpy array comrpising the EA values [length (6138)]\n",
        "\n",
        "#set up data splitting. Code is written for easy modification for cross-validation, but we ignore it here\n",
        "fold = 1\n",
        "n_splits = 1                                                                                                 # set only 1 fold\n",
        "train_idx, valid_idx = train_test_split(range(len(smiles)), test_size=0.1, random_state=31, shuffle=True)   # get indices"
      ],
      "metadata": {
        "id": "sBhtL9pRBJzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.3 Define dataloader"
      ],
      "metadata": {
        "id": "FpgBGbq_JeJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(X, y, train_index, valid_index, batch_size, fold_scaler):\n",
        "  # pull out the appropriate test and validation splits using the indexes\n",
        "  X_train, X_valid = X[train_index], X[valid_index]\n",
        "  y_train, y_valid = y[train_index], y[valid_index]\n",
        "\n",
        "  y_train_scaled = fold_scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "  y_valid_scaled = fold_scaler.transform(y_valid.reshape(-1, 1))\n",
        "\n",
        "  #Convert to tensors\n",
        "  X_train_tensor = torch.tensor(X_train)\n",
        "  X_valid_tensor = torch.tensor(X_valid)\n",
        "  y_train_tensor = torch.tensor(y_train_scaled)\n",
        "  y_valid_tensor = torch.tensor(y_valid_scaled)\n",
        "\n",
        "  #PyTorch DataLoader\n",
        "  train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "  valid_loader = DataLoader(TensorDataset(X_valid_tensor, y_valid_tensor), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "f1WON5hhEIyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 3.4 Train and validate model"
      ],
      "metadata": {
        "id": "q0CA9xyVNGww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define our training loop:\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X), y)                                     #average loss per sample in batch\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X.size(0)                             #batch loss = average loss in batch * batch size. Add batch loss to train_loss\n",
        "    return total_loss / len(loader.dataset)                               #after for loop, train_loss is the total loss over entire dataset. Calc average per sample by dividing by num_train\n",
        "\n",
        "# define our evaluation loop:\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            out = model(X)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "    return total_loss / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "lgI-GsEg2QbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for storing all predictions and targets, for all folds\n",
        "results = []\n",
        "# for per-fold analysis\n",
        "fold_r2_scores = []\n",
        "fold_mae_scores = []\n",
        "train_losses_dict = {}\n",
        "valid_losses_dict = {}\n",
        "\n",
        "#Per fold\n",
        "for fold, (train_idx, valid_idx) in enumerate([(train_idx, valid_idx)], 1):\n",
        "    # define (and reset) the model:\n",
        "    train_losses, valid_losses = [], []\n",
        "    fold_scaler = StandardScaler()\n",
        "    train_loader, valid_loader = dataloader(X, y, train_idx, valid_idx, batch_size, fold_scaler)\n",
        "\n",
        "    model = SimpleNN().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)                           # reduce the learning rate over time\n",
        "\n",
        "    best_valid_loss = float('inf')                                                                      # for early stopping\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    #train and evaluate:\n",
        "    pbar = tqdm(range(num_epochs), desc=f\"Fold {fold}\")                                                 # make a tqdm object so we have a progress bar\n",
        "    for epoch in pbar:\n",
        "        train_loss = ########################################                   # call the training loop\n",
        "        valid_loss = ########################################                   # call the eval loop\n",
        "        train_losses.append(train_loss)                                                                 # collect the per-fold, per-epoch losses for plotting\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        #Early stopping:\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            epochs_without_improvement = 0\n",
        "            best_model_state = model.state_dict()                                                        # save best weights\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                model.load_state_dict(best_model_state)                                                  # restore best weights if early stopping\n",
        "                break\n",
        "\n",
        "        #change learning rate and update progress bar\n",
        "        pbar.set_postfix({\"Train\": f\"{train_loss:.3f}\", \"Valid\": f\"{valid_loss:.3f}\", \"Best\": f\"{best_valid_loss:.3f}\"})\n",
        "        scheduler.step()\n",
        "\n",
        "    model.load_state_dict(best_model_state)                                                              # restore best weights if no early stopping\n",
        "    train_losses_dict[fold] = train_losses                                                               # store the per-fold, per-epoch losses with the key = fold number\n",
        "    valid_losses_dict[fold] = valid_losses\n",
        "\n",
        "    #Evaluate predictions and metrics per fold:\n",
        "    preds, targets = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in valid_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            preds.append(outputs.cpu())                                                                   # cpu as upcoming NumPy and scikit-learn don't work with GPU tensors\n",
        "            targets.append(batch_y.cpu())\n",
        "\n",
        "    #Put fold results in a temporary dataframe (gets overwritten)\n",
        "    fold_preds   = torch.cat(preds).numpy()                                                               # Concatenate all batch tensors into one NumPy array (NumPy as sklearn does not accept tensors)\n",
        "    fold_targets = torch.cat(targets).numpy()\n",
        "    fold_preds_unscaled = fold_scaler.inverse_transform(fold_preds.reshape(-1, 1))                        # Apply inverse scaling. reshape as sklearn wants 2D arrays.\n",
        "    fold_targets_unscaled = fold_scaler.inverse_transform(fold_targets.reshape(-1, 1))\n",
        "\n",
        "    df_fold = pd.DataFrame({\n",
        "      \"original_index\": valid_idx,                                                                        # keep original indices, as we shuffled during the KFold but want to match to original smiles list\n",
        "      \"preds\": fold_preds_unscaled.flatten(),                                                             # (flatten to 1D array)\n",
        "      \"targets\": fold_targets_unscaled.flatten(),\n",
        "      \"fold\": fold\n",
        "    })\n",
        "    results.append(df_fold)                                                                               #Save predictions and targets alongside fold number across all folds for later plotting\n",
        "\n",
        "    #Compute metrics per fold\n",
        "    r2 = r2_score(df_fold[\"targets\"], df_fold[\"preds\"])\n",
        "    mae = mean_absolute_error(df_fold[\"targets\"], df_fold[\"preds\"])\n",
        "    fold_r2_scores.append(r2)                                                                             # collect the per-fold r2 into a single list\n",
        "    fold_mae_scores.append(mae)\n",
        "    print(f\"R²={r2:.3f}, MAE={mae:.3f}\\n\")\n",
        "\n",
        "#After all folds:\n",
        "#=================\n",
        "# save all predictions in a dataframe\n",
        "df_results = pd.concat(results, ignore_index=True)\n",
        "df_all = df_data.merge(df_results, left_index=True, right_on=\"original_index\")\n",
        "\n",
        "# Aggregate across folds\n",
        "mean_r2 = np.mean(fold_r2_scores)\n",
        "mean_mae = np.mean(fold_mae_scores)\n",
        "std_r2 = np.std(fold_r2_scores)\n",
        "std_mae = np.std(fold_mae_scores)\n",
        "print(\"\\nFull results:\")\n",
        "print(f\"Mean R²: {mean_r2:.3f} ± {std_r2:.3f}\")\n",
        "print(f\"Mean MAE: {mean_mae:.3f} ± {std_mae:.3f}\")"
      ],
      "metadata": {
        "id": "6MV-wea1L126"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.5 Plot Results"
      ],
      "metadata": {
        "id": "87t-kjPFYcv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all)"
      ],
      "metadata": {
        "id": "wgdOAoZG4XrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots\n",
        "n_plots = n_splits + 2  # train/val plots + scatter + bar\n",
        "fig, axes = plt.subplots(3, 4, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Training vs Validation Loss per fold\n",
        "for fold in range(1, n_splits+1):\n",
        "    ax = axes[fold-1]\n",
        "    ax.plot(train_losses_dict[fold], label=f\"Train Fold {fold}\", alpha=0.7)\n",
        "    ax.plot(valid_losses_dict[fold], label=f\"Valid Fold {fold}\", alpha=0.7)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"MSE Loss\")\n",
        "    ax.set_title(f\"Fold {fold} Losses\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.legend()\n",
        "\n",
        "# Scatter plot: True vs Predicted\n",
        "ax2 = axes[n_splits]\n",
        "for fold in range(1, n_splits+1):\n",
        "    fold_data = df_all[df_all[\"fold\"] == fold]\n",
        "    ax2.scatter(\n",
        "        fold_data[\"targets\"],\n",
        "        fold_data[\"preds\"],\n",
        "        alpha=0.8,\n",
        "        s=5,\n",
        "        label=f\"Fold {fold}\"\n",
        "    )\n",
        "\n",
        "ax2.set_xlabel(\"True EA\")\n",
        "ax2.set_ylabel(\"Predicted EA\")\n",
        "ax2.set_title(\"True vs Predicted EA\")\n",
        "lims = [\n",
        "    min(df_all[\"targets\"].min(), df_all[\"preds\"].min()),\n",
        "    max(df_all[\"targets\"].max(), df_all[\"preds\"].max())\n",
        "]\n",
        "ax2.plot(lims, lims, \"r--\")  # y=x reference\n",
        "ax2.text(0.05, 0.95, f\"R²: {mean_r2:.3f} ± {std_r2:.3f}\",\n",
        "         transform=ax2.transAxes, fontsize=10, verticalalignment='top')\n",
        "ax2.text(0.05, 0.85, f\"MAE: {mean_mae:.2f} ± {std_mae:.2f}\",\n",
        "         transform=ax2.transAxes, fontsize=10, verticalalignment='top')\n",
        "ax2.legend(title=\"Folds\", markerscale=2, loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
        "\n",
        "# Bar chart: MAE per fold\n",
        "ax3 = axes[n_splits+1]\n",
        "cmap = plt.cm.get_cmap(\"tab10\")\n",
        "colors = [cmap(fold-1) for fold in range(1, n_splits+1)]\n",
        "ax3.bar(range(1, n_splits+1), fold_mae_scores, color=colors, edgecolor=\"black\")\n",
        "ax3.set_xlabel(\"Fold\")\n",
        "ax3.set_ylabel(\"MAE\")\n",
        "ax3.set_title(\"MAE per Fold\")\n",
        "ax3.set_xticks(range(1, n_splits+1))\n",
        "\n",
        "# Delete any unused subplots\n",
        "for i in range(n_splits+2, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zZPFk34FJ7VB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}